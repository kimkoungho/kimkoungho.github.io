




# 아파치 카프카 란?

## 카프카의 탄생 
카프카는 2011년 링크드인에서 개발됨 
당시의 링크드인 에서는 파편화된 데이터 수집 및 분배를 위한 아키텍처를 운영하는데 큰 어려움이 있었음 

- 일반적으로 데이터 적재 하기 위해서는 **데이터를 생성하는 소스 애플리케이션** 과 **데이터를 저장하는 타깃 애플리케이션** 의 연결이 필요함 
    * 소스 애플리케이션 : 웹, 앱, 백엔드 서버 등 
    * 타깃 애플리케이션 : DB, Hadoop, ES 등 
- 쉬운 방법으로 end-to-end 방식으로 연결 할 수 있지만, 
- 시간이 지날수록 아키텍쳐가 거대해져서 소스 애플리케이션 과 타깃 애플리케이션 많아지면서 데이터를 전송하는 라인이 복잡해졌음 
- 그리고 데이터를 저장하는 파이프 라인이 많아 지면서 소스 코드 및 버전 관리의 어려움이 있었음 
- 결국 링크드인의 데이터팀은 신규시스템인 아파치 카프카를 개발하게 됨 

빅데이터 관련 내용이랑 좀 겹치는데 ..


- 데이터 레이크(data lake) 
빅데이터를 활용하기 위해서는 일단 생성되는 데이터를 모두 모으는 것이 중요한데, 이때 사용되는것이 데이터 레이크 
데이터 레이크는 데이터가 모이는 저장 공간을 의미함 
데이터 웨어하우스와는 다르게 필터링되거나 패키지화 되지 않은 데이터가 저장된다 


서비스에서 발생하는 데이터를 데이터 레이크로 모으기 위한 방법은 뭐가 있을까 ?
- end-to-end 로 바로 연결하는 방법 
이 방법은 서비스의 규모가 크지 않으면 손쉽게 구현할 수 있지만, 서비스의 규모가 커질 수 록 복잡도가 올라가서 관리가 어렵다는 단점이 있음 
- 데이터 파이프 라인을 이용하는 방법
end-to-end 방식의 복잡도가 높은 문제를 해결하기 위한 방법으로 데이터의 추출(extracting), 변경(transforming), 적재(loading) 하는 과정을 파이프 라인으로 구축하는 방법 

*** 이런 데이터 파이프 라인을 안정적이고 확장성 높게 운영하기 위한 좋은 방법이 카프카를 활용하는 것 


## 카프카 프로듀서, 컨슈머, 토픽 모식도 
.. 그림 ..

프로듀서
- 토픽
카프카에 전달할 수 있는 데이터 포맷은 제한이 없음 
직렬화, 역직렬화를 이용하기 때문에 자바에서 선언 가능한 모든 객체를 지원함 

컨슈머 

## 

## 카프카의 특징 
- 오픈소스
2011 년에 오픈소스로 공개되어 관련 생태계가 매우 넓어짐 
카프카 오픈소스는 [github](https://github.com/apache/kafka) 에 공개되어 누구나 내부동작을 확인하거나 이슈를 제기 할 수 있음 
> KIP(Kafka Improvement Proposal) 는 카프카의 주요 변경사항을 제안하는 방법으로 누구든 생성할 수 있음 
> 카프카의 주요 아키텍쳐 개선과 같은 ... KIP 를 제안하는 방법은 https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals 에서 확인하자 

- 높은 처리량
카프카는 프로듀서가 브로커로 데이터를 보낼때, 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다 
네트워크 상에서 같은 데이터를 전송할 때, 네트워크 통신 횟수를 줄이면 더 많은 데이터를 전송 가능하기 때문에 데이터를 묶어서 전송하여 높은 처리량을 보장함 
또한, 파티션을 지원하여 동일 목적의 데이터를 여러 파티션에 분배하고 병렬 처리가 가능함 

- 확장성
카프카는 안정적으로 확장 가능하도록 설계되어있다 
데이터가 늘어났을때 scale-out을 하거나 데이터가 줄어들었을때 scale-in 필요할 수 있는데 카프카 클러스터는 scale-out과 scale-in 을 무중단으로 지원

- 영속성 
카프카는 다른 메시징 플랫폼과 다르게 데이터를 메모리가 아닌 파일 시스템에 저장
ㄴ 다른 메시징 시스템인 MQ 의 경우에도 파일 시스템 모드를 지원하긴 함 
파일 시스템에 메시지를 저장하기 때문에 장애 발생시에도 프로세스 재시작 만으로 안전하게 데이터를 다시 처리 가능 

파일 시스템을 이용하기 때문에 일반적으로 느리다고 생각 할 수 있는데, 
카프카는 운영체제 레벨에서 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 생성하여 이를 개선했다 
페이지 캐시 메모리 영역을 사용해서 한 번 읽은 파일 내용은 메모리에 저장 시켰다가 다시 사용하는 방식으로 빠른 응답이 가능 
ㄴ 사실 메모리 보다 빠르진 않을 것 같음 

- 고가용성 
클러스터로 이루어진 카프카는 데이터의 복제를 통해서 고가용성을 보장한다 
프로듀서로 전달받은 데이터를 1개의 브로커에만 저장하는 것이 아니라 다른 브로커들에도 저장함으로써 
특정 브로커에 장애가 발생해도 다른 브로커에 데이터가 저장되어 있기 때문에 데이터를 유실하지 않을 수 있다 
그리고 카프카는 클라우드 리전 단위의 장애나 온프로미스 환경의 서버 렉 단위의 장애에도 데이터를 유지 할 수 있는 솔루션을 갖고 있다 

> 카프카 클러스터는 최소 3개의 브로커를 권장하는 데, 이것은 고가용성과 관련이 있다 
> 카프카에는 min.insync.replicas 라는 옵션이 있는데 데이터를 저장하는 브로커의 수를 지정하는 것이다
> 기본값은 2 인데, 카프카 1대에 장애가 발생할 경우 적어도 2대의 브로커가 있어야 메시지가 저장된다
> 클러스터 내에 브로커가 1대만 존재하면 토픽을 저장할 수 없음 


## 데이터 레이크 아키텍쳐 
데이터 레이크 아키텍쳐는 2가지가 있음 
- 람다(lambda) 아키텍쳐 
- 카파(kappa) 아키텍쳐

### 람다 아키텍쳐 

- 레거시 아키텍처

그림 ..

초기 빅데이터 플랫폼은 end-to-end 로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았음 

단점 
- 실시간으로 생성되는 데이터들에 대한 인사이트(데이터 분석 결과?)를 실시간으로 제공 못함
- 파생된 데이터의 히스토리 파악이 어려움 
- 데이터 가공으로 데이터가 파편화 되면서 **데이터 거버넌스**를 지키기 어려움


- 람다 아키텍처 
레거시 데이터 수집 플랫폼을 개선하기 위해 구성된 아키텍쳐 

배치 데이터를 처리하는 부분외에 스피드 레이어 라는 실시간 데이터 ETL 작업 영역을 정의 
그림 ..

람다 아키텍처는 3가지 레이어로 구성됨 
- 배치 레이어 
배치 데이터를 모아서 특정 시간, 타이밍 마다 일괄 처리를 하는 레이어 
- 서빙 레이어 
가공된 데이터를 사용사, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간 
- 스피드 레이어 
서비스에서 생성 되는 실시간 원천 데이터를 분석하는 용도로 사용
실시간 데이터는 가공해서 바로 사용자나 서비스를 전달할 수 도 있고 필요한 경우 서빙 레이어에 저장한다 

람다 아키텍처에서 카프카는 스피드 레이어에 위치하여 실시간 데이터를 처리, 분석 한다 
카프카 스트립즈와 같은 스트림 프로세싱 도구는 윈도우 함수, 상태 기반 프로세싱, 무상태 기반 프로세싱 등의 분석을 위한 다양한 기능을 제공하여 람다 아키텍처의 중요 플랫폼으로 자리잡았음 


- 한계 
람다 아키텍처는 배치 레이어와 스피드 레이어를 분리하여 데이터 처리 방식을 나눌수 있었지만 
레이어가 2개로 나뉘기 때문에 데이터 분석, 처리를 위한 로직이 2벌로 존재해야하는 한계를 갖는다 
또한 배치 레이어와 스피드 레이어를 융합하여 처리하기 쉽지 않다는 단점이 있다 

- 카파 아키텍처
이러한 람다 아키텍처의 단점을 보안하기 위해서 제이 크랩스(Jay Kreps: 카프카를 최초로 고안한 개발자, 전 링크드인 팀자, 현 컨플루언트 CEO) 는 카파 아키첵처를 제안했음 
배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣어서 처리하도록 구성하는 방식 
스피드 레이어에서 모든 데이터를 처리하기 때문에 서비스에서 생성하는 모든 데이터를 스트림 처리 해야함 

데이터 플랫폼에서 배치 데이터는 일반적으로 해당 시점의 데이터들의 스냅샷임 
이런 배치 데이터를 각 시점의 변환 기록을 순서대로 기록 함으로써 각 시점에 대한 데이터를 저장하지 않고도 배치 데이터를 표현 가능하게 된다
ㄴ 즉, 데이터 변경여부를 트래킹 하기 때문에 시간에 관계 없이 특정 시점에 해당 데이터가 무엇이었는지 추측 가능하다는 얘기 

카파 아키텍쳐에서는 모든 데이터가 스피드 레이어로 들어오기 때문에 스피드 레이어를 구성하는 데이터 플랙폼은 SPOF(Single Point Of Failure) 가 될 수 있기 때문에 
반드시 내결함성(High Availability) 와 장애 허용(Fault Tolerant) 특성을 지녀야 한다 
이러한 특성을 카프카는 이미 갖고 있기 때문에 좋은 솔루션 이다 
> 카프카에서 사용하고 있는 파티션, 레코드, 오프셋의 개념은 제이 크립스가 고안한 로그 데이터 플랫폼의 구현체로 볼 수 있다 


- 최근 동향 
2020년 카프카 서밋에서 제이 크랩스는 카파 아키텍처에서 서빙 레이어를 제거한 아키텍처인 **스트리밍 데이터 레이크** 를 제안 
서빙 레이어는 일반적으로 하둡 파일 시스템, 오브젝트 스토리지와 같이 데이터 플랫폼에서 흔히 사용하는 저장소인데 
카프카를 이미 사용하고 있는 상황에서 굳이 프로세싱한 데이터를 다시 서빙 레이어에 저장할 필요가 있을까 ? 라는 의문에서 이런 동향이 시작되었다 
ㄴ 카프카 이미 파일 시스템에 메시지를 저장하기 때문에 영구 보존이 가능하다는 것을 강조하는 내용인듯 하다 
카프카는 데이터를 영구 저장하기 때문에 굳이 서빙레이어가 필요하지 않으며, 서빙 레이어의 존재는 데이터를 이중으로 저장함으로써 생기는 데이터 불일치나 동기화 같은 문제들을 야기 할 수 있다 

하지만 카프카를 스트리밍 데이터 레이크로 사용하기에는 몇 가지 문제점들이 존재하는데, 
- 자주 접근하지 않는 데이터들은 굳이 비싼 자원 (페이지 캐시)에 유지 할 필요가 없음 
- 자주 접근하지 않는 데이터들은 오브젝트 스토리지와 같은 저렴하면서도 안정한 저장소에 저장하는 게 리소스의 낭비를 막을 수 있음 
이런 문제를 해결하기 위해서 카프카도 단계별 저장소를 가질 수 있도록 추가기능을 개발하고 있다 

추가적으로 카프카 사용시에도 데이터를 쿼리 할 수 있는 데이터 플랫폼이 필요할 수 있는데, 컨플루언트에서는 SQL 기반으로 카프카를 조회할 수 있도록 ksqlDB 를 오픈소스로 제공한다 
ksqlDB 는 아직 타임 스탬프, 오프셋, 파티션 기반의 쿼리를 사용하지 못하기 때문에 완벽하게 query 를 날리는것 에는 제약이 존재함 







