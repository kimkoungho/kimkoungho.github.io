




# 아파치 카프카 란?

## 카프카의 탄생 
카프카는 2011년 링크드인에서 개발됨 
당시의 링크드인 에서는 파편화된 데이터 수집 및 분배를 위한 아키텍처를 운영하는데 큰 어려움이 있었음 

- 일반적으로 데이터 적재 하기 위해서는 **데이터를 생성하는 소스 애플리케이션** 과 **데이터를 저장하는 타깃 애플리케이션** 의 연결이 필요함 
    * 소스 애플리케이션 : 웹, 앱, 백엔드 서버 등 
    * 타깃 애플리케이션 : DB, Hadoop, ES 등 
- 쉬운 방법으로 end-to-end 방식으로 연결 할 수 있지만, 
- 시간이 지날수록 아키텍쳐가 거대해져서 소스 애플리케이션 과 타깃 애플리케이션 많아지면서 데이터를 전송하는 라인이 복잡해졌음 
- 그리고 데이터를 저장하는 파이프 라인이 많아 지면서 소스 코드 및 버전 관리의 어려움이 있었음 
- 결국 링크드인의 데이터팀은 신규시스템인 아파치 카프카를 개발하게 됨 

빅데이터 관련 내용이랑 좀 겹치는데 ..


- 데이터 레이크(data lake) 
빅데이터를 활용하기 위해서는 일단 생성되는 데이터를 모두 모으는 것이 중요한데, 이때 사용되는것이 데이터 레이크 
데이터 레이크는 데이터가 모이는 저장 공간을 의미함 
데이터 웨어하우스와는 다르게 필터링되거나 패키지화 되지 않은 데이터가 저장된다 


서비스에서 발생하는 데이터를 데이터 레이크로 모으기 위한 방법은 뭐가 있을까 ?
- end-to-end 로 바로 연결하는 방법 
이 방법은 서비스의 규모가 크지 않으면 손쉽게 구현할 수 있지만, 서비스의 규모가 커질 수 록 복잡도가 올라가서 관리가 어렵다는 단점이 있음 
- 데이터 파이프 라인을 이용하는 방법
end-to-end 방식의 복잡도가 높은 문제를 해결하기 위한 방법으로 데이터의 추출(extracting), 변경(transforming), 적재(loading) 하는 과정을 파이프 라인으로 구축하는 방법 

*** 이런 데이터 파이프 라인을 안정적이고 확장성 높게 운영하기 위한 좋은 방법이 카프카를 활용하는 것 


## 카프카 프로듀서, 컨슈머, 토픽 모식도 
.. 그림 ..

프로듀서
- 토픽
카프카에 전달할 수 있는 데이터 포맷은 제한이 없음 
직렬화, 역직렬화를 이용하기 때문에 자바에서 선언 가능한 모든 객체를 지원함 

컨슈머 

## 

## 카프카의 특징 
- 오픈소스
2011 년에 오픈소스로 공개되어 관련 생태계가 매우 넓어짐 
카프카 오픈소스는 [github](https://github.com/apache/kafka) 에 공개되어 누구나 내부동작을 확인하거나 이슈를 제기 할 수 있음 
> KIP(Kafka Improvement Proposal) 는 카프카의 주요 변경사항을 제안하는 방법으로 누구든 생성할 수 있음 
> 카프카의 주요 아키텍쳐 개선과 같은 ... KIP 를 제안하는 방법은 https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals 에서 확인하자 

- 높은 처리량
카프카는 프로듀서가 브로커로 데이터를 보낼때, 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다 
네트워크 상에서 같은 데이터를 전송할 때, 네트워크 통신 횟수를 줄이면 더 많은 데이터를 전송 가능하기 때문에 데이터를 묶어서 전송하여 높은 처리량을 보장함 
또한, 파티션을 지원하여 동일 목적의 데이터를 여러 파티션에 분배하고 병렬 처리가 가능함 

- 확장성
카프카는 안정적으로 확장 가능하도록 설계되어있다 
데이터가 늘어났을때 scale-out을 하거나 데이터가 줄어들었을때 scale-in 필요할 수 있는데 카프카 클러스터는 scale-out과 scale-in 을 무중단으로 지원

- 영속성 
카프카는 다른 메시징 플랫폼과 다르게 데이터를 메모리가 아닌 파일 시스템에 저장
ㄴ 다른 메시징 시스템인 MQ 의 경우에도 파일 시스템 모드를 지원하긴 함 
파일 시스템에 메시지를 저장하기 때문에 장애 발생시에도 프로세스 재시작 만으로 안전하게 데이터를 다시 처리 가능 

파일 시스템을 이용하기 때문에 일반적으로 느리다고 생각 할 수 있는데, 
카프카는 운영체제 레벨에서 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 생성하여 이를 개선했다 
페이지 캐시 메모리 영역을 사용해서 한 번 읽은 파일 내용은 메모리에 저장 시켰다가 다시 사용하는 방식으로 빠른 응답이 가능 
ㄴ 사실 메모리 보다 빠르진 않을 것 같음 

- 고가용성 
클러스터로 이루어진 카프카는 데이터의 복제를 통해서 고가용성을 보장한다 
프로듀서로 전달받은 데이터를 1개의 브로커에만 저장하는 것이 아니라 다른 브로커들에도 저장함으로써 
특정 브로커에 장애가 발생해도 다른 브로커에 데이터가 저장되어 있기 때문에 데이터를 유실하지 않을 수 있다 
그리고 카프카는 클라우드 리전 단위의 장애나 온프로미스 환경의 서버 렉 단위의 장애에도 데이터를 유지 할 수 있는 솔루션을 갖고 있다 

> 카프카 클러스터는 최소 3개의 브로커를 권장하는 데, 이것은 고가용성과 관련이 있다 
> 카프카에는 min.insync.replicas 라는 옵션이 있는데 데이터를 저장하는 브로커의 수를 지정하는 것이다
> 기본값은 2 인데, 카프카 1대에 장애가 발생할 경우 적어도 2대의 브로커가 있어야 메시지가 저장된다
> 클러스터 내에 브로커가 1대만 존재하면 토픽을 저장할 수 없음 


## 데이터 레이크 아키텍쳐 
데이터 레이크 아키텍쳐는 2가지가 있음 
- 람다(lambda) 아키텍쳐 
- 카파(kappa) 아키텍쳐

### 람다 아키텍쳐 

- 레거시 아키텍처

그림 ..

초기 빅데이터 플랫폼은 end-to-end 로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았음 

단점 
- 실시간으로 생성되는 데이터들에 대한 인사이트(데이터 분석 결과?)를 실시간으로 제공 못함
- 파생된 데이터의 히스토리 파악이 어려움 
- 데이터 가공으로 데이터가 파편화 되면서 **데이터 거버넌스**를 지키기 어려움


- 람다 아키텍처 
레거시 데이터 수집 플랫폼을 개선하기 위해 구성된 아키텍쳐 

배치 데이터를 처리하는 부분외에 스피드 레이어 라는 실시간 데이터 ETL 작업 영역을 정의 
그림 ..

람다 아키텍처는 3가지 레이어로 구성됨 
- 배치 레이어 
배치 데이터를 모아서 특정 시간, 타이밍 마다 일괄 처리를 하는 레이어 
- 서빙 레이어 
가공된 데이터를 사용사, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간 
- 스피드 레이어 
서비스에서 생성 되는 실시간 원천 데이터를 분석하는 용도로 사용
실시간 데이터는 가공해서 바로 사용자나 서비스를 전달할 수 도 있고 필요한 경우 서빙 레이어에 저장한다 

람다 아키텍처에서 카프카는 스피드 레이어에 위치하여 실시간 데이터를 처리, 분석 한다 
카프카 스트립즈와 같은 스트림 프로세싱 도구는 윈도우 함수, 상태 기반 프로세싱, 무상태 기반 프로세싱 등의 분석을 위한 다양한 기능을 제공하여 람다 아키텍처의 중요 플랫폼으로 자리잡았음 


- 한계 
람다 아키텍처는 배치 레이어와 스피드 레이어를 분리하여 데이터 처리 방식을 나눌수 있었지만 
레이어가 2개로 나뉘기 때문에 데이터 분석, 처리를 위한 로직이 2벌로 존재해야하는 한계를 갖는다 
또한 배치 레이어와 스피드 레이어를 융합하여 처리하기 쉽지 않다는 단점이 있다 

- 카파 아키텍처
이러한 람다 아키텍처의 단점을 보안하기 위해서 제이 크랩스(Jay Kreps: 카프카를 최초로 고안한 개발자, 전 링크드인 팀자, 현 컨플루언트 CEO) 는 카파 아키첵처를 제안했음 
배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣어서 처리하도록 구성하는 방식 
스피드 레이어에서 모든 데이터를 처리하기 때문에 서비스에서 생성하는 모든 데이터를 스트림 처리 해야함 

데이터 플랫폼에서 배치 데이터는 일반적으로 해당 시점의 데이터들의 스냅샷임 
이런 배치 데이터를 각 시점의 변환 기록을 순서대로 기록 함으로써 각 시점에 대한 데이터를 저장하지 않고도 배치 데이터를 표현 가능하게 된다
ㄴ 즉, 데이터 변경여부를 트래킹 하기 때문에 시간에 관계 없이 특정 시점에 해당 데이터가 무엇이었는지 추측 가능하다는 얘기 

카파 아키텍쳐에서는 모든 데이터가 스피드 레이어로 들어오기 때문에 스피드 레이어를 구성하는 데이터 플랫폼은 SPOF(Single Point Of Failure) 가 될 수 있기 때문에 
반드시 내결함성(High Availability) 와 장애 허용(Fault Tolerant) 특성을 지녀야 한다 
이러한 특성을 카프카는 이미 갖고 있기 때문에 좋은 솔루션 이다 
> 카프카에서 사용하고 있는 파티션, 레코드, 오프셋의 개념은 제이 크립스가 고안한 로그 데이터 플랫폼의 구현체로 볼 수 있다 


- 최근 동향 
2020년 카프카 서밋에서 제이 크랩스는 카파 아키텍처에서 서빙 레이어를 제거한 아키텍처인 **스트리밍 데이터 레이크** 를 제안 
서빙 레이어는 일반적으로 하둡 파일 시스템, 오브젝트 스토리지와 같이 데이터 플랫폼에서 흔히 사용하는 저장소인데 
카프카를 이미 사용하고 있는 상황에서 굳이 프로세싱한 데이터를 다시 서빙 레이어에 저장할 필요가 있을까 ? 라는 의문에서 이런 동향이 시작되었다 
ㄴ 카프카 이미 파일 시스템에 메시지를 저장하기 때문에 영구 보존이 가능하다는 것을 강조하는 내용인듯 하다 
카프카는 데이터를 영구 저장하기 때문에 굳이 서빙레이어가 필요하지 않으며, 서빙 레이어의 존재는 데이터를 이중으로 저장함으로써 생기는 데이터 불일치나 동기화 같은 문제들을 야기 할 수 있다 

하지만 카프카를 스트리밍 데이터 레이크로 사용하기에는 몇 가지 문제점들이 존재하는데, 
- 자주 접근하지 않는 데이터들은 굳이 비싼 자원 (페이지 캐시)에 유지 할 필요가 없음 
- 자주 접근하지 않는 데이터들은 오브젝트 스토리지와 같은 저렴하면서도 안정한 저장소에 저장하는 게 리소스의 낭비를 막을 수 있음 
이런 문제를 해결하기 위해서 카프카도 단계별 저장소를 가질 수 있도록 추가기능을 개발하고 있다 

추가적으로 카프카 사용시에도 데이터를 쿼리 할 수 있는 데이터 플랫폼이 필요할 수 있는데, 컨플루언트에서는 SQL 기반으로 카프카를 조회할 수 있도록 ksqlDB 를 오픈소스로 제공한다 
ksqlDB 는 아직 타임 스탬프, 오프셋, 파티션 기반의 쿼리를 사용하지 못하기 때문에 완벽하게 query 를 날리는것 에는 제약이 존재함 
이외에도 아파치 스파크 스트리밍, 프레스토, 아피치 드릴, 하이브/스파크 SQL 등을 조합하는 방식도 **스트리밍 데이터 레이크** 로 구성하는 방식이다 




## 카프카 브로커
카프카 클라이언트와 데이터를 주고받기 위해서 사용하는 카프카의 본체 
데이터를 분산 저장하여 장애를 발생해도 안전하게 사용할 수 있도록 도와주는 애플리케이션 

- 데이터 저장 및 전송
카프카 브로커는 프로듀서에서 데이터를 전달받으면 요청한 토픽의 파티션에 데이터를 저장 
컨슈머가 데이터를 요청하면 해당 파티션에 데이터를 전달한다 

저장된 데이터 조회 - VM 장비 
```bash
# config/server.properties 에 지정된 log.dir 디렉토리에 데이터를 저장 
$ ls /tmp/kafka-logs
# 파티션이 4개 여서 4개로 출력됨 
hello.kafka-0 hello.kafka-1 hello.kafka-2 hello.kafka-3
...

# 0번 파티션의 데이터 조회 
$ ls /tmp/kafka-logs/hello.kafka-0/
00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  leader-epoch-checkpoint
# - index : 오프셋을 인덱싱한 정보
# - timeindex : 메시지에 포함된 timestamp 값이 포함됨 
```

카프카는 데이터를 파일 시스템에 저장하지만, **페이지 캐시** 를 이용하기 때문에 디스크 입출력 속도를 높여서 처리속도를 개선
> 페이지 캐시는 OS 에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역
> 한번 읽은 파일의 내용은 디스크에서 읽지 않고 메모리에서 직접 읽는다 
> 카프카는 페이지 캐시를 이용하기 때문에 힙 메모리 사이즈를 크게 설정할 필요가 없음 
> ㄴ 만약 페이지 캐시를 사용하지 않고 직접 캐시를 구현한다면 지속적으로 변경되는 데이터로 인해서 GC 가 자주 일어나 성능이 느려질 수 있음 

- 데이터 복제, 싱크 
데이터 복제(replication) 는 카프카를 장애 허용 시스템(fault tolerant system) 으로 동작하게 해준다 
복제를 하는 이유는 클러스터에 있는 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않기 위함 
카프카에서는 파티션 단위로 데이터를 복제한다 
토픽 생성시에 replication factor 를 지정하여 복제 수를 지정 (기본값: 1)

데이터 복제 관점에서 파티션은 **리더 파티션** 과 **팔로워 파티션** 으로 나뉜다 
* 리더 파티션 : 프로듀서, 컨슈머와 직접 소통하는 파티션 (원본 데이터)
* 팔로워 파티션 : 복제 데이터를 넣는 용도의 파티션 
팔로워 파티션들은 리더 파티션의 **오프셋** 을 확인하여 현재 자신의 오프셋과 비교 후 데이터를 저장함 -> 싱크 방법 

리더 파티션에 장애가 발생한다면, 팔로워 파티션 중 1개가 리더 파티션이 된다 

- 컨트롤러
클러스터 내의 1대의 브로커가 컨트롤러 역할을 함 
컨트롤러는 다른 브로커들의 **상태를 체크** 하고 특정 브로커가 클러스터에 빠지는 경우 **리더 파티션** 을 재분배 한다 
만약 컨트롤러 역할을 하는 브로커에 장애가 발생하면 다른 브로커가 컨트롤러가 된다 

- 데이터 삭제 
카프카는 컨슈머가 데이터를 가져가더라도 데이터를 삭제하지 않음 
컨슈머나 프로듀서가 데이터 삭제를 요청할 수 없고 브로커만이 데이터를 삭제한다 
데이터 삭제는 **로그 세그먼트** 라는 파일 단위로 삭제가 일어나며 여러 데이터를 함께 삭제함
ㄴ 임의의 레코드만 삭제하는 것은 불가능 

- 컨슈머 오프셋 저장
컨슈머 그룹은 토픽의 파티션에서 데이터를 가져가고 어느 레코드까지 가져갔는지 **오프셋을 커밋** 한다 
커밋한 오프셋은 __consumer_offsets 토픽에서 확인할 수 있다 

- 코디네이터 
클러스터 내의 1대의 브로커가 코디네이터 역할을 수행 
코디네이터는 컨슈머의 그룹의 상태를 체크하고 **파티션을 컨슈머와 매팅되도록 분배** 역할을 함 
컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 다른 컨슈머로 할당한다 
파티션을 컨슈머로 재할당하는 과정을 **리밸런스** 라고 부른다 

## 주키퍼 
주키퍼는 카프카의 메타 데이터를 관리하는 데에 사용됨 

주키퍼에 저장된 카프카의 메타 데이터 확인해보기 
```bash
# 주키퍼 쉘을 이용해서 주키퍼에 저장된 znode 를 확인할 수 있음 
$ bin/zookeeper-shell.sh my-kafka:2181
Welcome to ZooKeeper!
JLine support is disabled

# root 노드 조회 
ls /
[admin, brokers, cluster, config, consumers, controller, controller_epoch, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]

# 카프카 브로커의 정보 조회 
get /brokers/ids/0
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://{VM_IP}:9092"],"jmx_port":-1,"host":"{VM_IP}","timestamp":"1620190316477","port":9092,"version":4}
# 컨트롤러 브로커 조회 
get /controller
{"version":1,"brokerid":0,"timestamp":"1620190316631"}
# 토픽 조회 
ls /brokers/topics
[__consumer_offsets, hello.kafka]
```

[71 페이지 이미지]()
카프카 클러스터로 묶인 브로커 들은 동일한 경로의 주키퍼 znode path 에 선언되어야 카프카 브로커 묶음이 된다 
ㄴ 클러스터 제약 인듯 

## 토픽과 파티션 
### 토픽
카프카에서 데이터를 구분하기 위해서 사용하는 데이터 단위 
토픽은 1개 이상의 파티션이 존재하고 프로듀서가 보낸 데이터를 브로커는 파티션에 데이터를 저장함 
브로듀서에서 보낸 데이터를 **레코드** 라고 부른다 

### 파티션
파티션은 실제로 데이터가 저장되는 물리적인 공간
카프카 병렬 처리의 핵심으로 컨슈머들이 레코드를 병렬로 처리 할 수 있도록 매칭함 
파티션은 자료구조의 Queue 와 비슷한 구조이며 FIFO 이다 
파티션의 데이터는 삭제 되지 않기 때문에 여러 컨슈머 그룹을 지정하면 데이터를 각각 가져갈 수 있다 

### 토픽의 이름 제약 조건 
- 빈 문자열 토픽 불가능 
- 토픽 이름은 마침표만으로 생성 불가 (., ..)
- 토픽 이름은 최대 249자
- 영어 대소문자와 숫자, 마침표(.), 언더바(_), 하이프(-) 조합 가능
- 카프카가 이미 사용중인 토픽이름 등록 불가 (__consumer_offsets, __transaction_state)
- 마침표(.) 와 언더바(_) 를 동시에 사용하는 이름은 카프카에서 warning 메시지가 발생 
- 마침표(.) 와 언더바(_) 만 다른 토픽은 생성 불가 
    * kafka.hello 가 존재하면 kafka_hello 는 생성 불가능 

### 토픽 이름 작명 예시 
- <환경>.<팀-명>.<애플리케이션-명>.<메시지-타입>
    * prd.marketing-team.sms-platform.json
- <프로젝트-명>.<서비스-명>.<환경>.<이벤트-명>
    * commerce.payment.prd.notification
- <환경>.<서비스-명>.<JIRA-번호>.<메시지-타입>
    * dev.email-sender.jira-1234.email-vo-custom
- <카프카-클러스터-명>.<환경>.<서비스-명>.<메시지-타입>
    * aws-kafka.live.marketing-platform.json

## 레코드 
레코드는 타임 스탬프, 메시지 키, 메시지 값, 오프셋으로 구성됨 

- 타임 스탬프: 브로커 기준 유닉스 시간이 설정 
    * 다만 프로듀서가 레코드를 생성할 때 임의의 타임스탬프를 지정할 수 있음 
- 메시지 키 : 메시지 값을 순서대로 처리하거나 메시지 값의 종류를 표현할 때 사용 
    * 브로커는 메시지 키를 해시하여 파티션을 결정함 - 해시 테이블 
    * 따라서 같은 메시지 키는 같은 파티션에 저장된다 -> 파티션이 추가되거나 제거되면 변할 수 있음 
- 메시지 키와 값은 직력화되어 저장되어 역직렬화 되어 처리된다 
- 오프셋 : 0 이상의 숫자로 이루어지며 직접 지정할 수 없다 
    * 레코드가 저장될 때 이전 offset + 1 로 저장 
    * 컨슈머 그룹이 어느정도의 데이터를 가져갔는지 명확히 알 수 있음 


# 카프카 클라이언트 
카프카에 명령을 내리거나 데이터 송수신을 위해서 카프카 클라이언트를 이용할 수 있음 
ㄴ CLI, java 라이브러리 등 


## 프로듀서 API 
프로듀서 애플리케이션은 카프카에 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 데이터를 전송함 

### 프로듀서 API 동작 방식 
[86 페이지 그림]()
- KafkaProducer 의 send 메소드를 호출하면 파티셔너에 의해서 어느 파티션으로 전송될 것인지 결정된다 
- 파티셔너에 의해 구분된 레코드는 데이터를 바로 전송하지 않고 Accumulator 에 데이터 버퍼로 쌓아두고 배치 형태로 전송된다 
- 배치 형태로 데이터를 전송하기 때문에 프로듀서의 처리량을 향상시키는데 도움이 된다 
- 기본 파티셔너는 DefaultPartitioner 이다 


- UniformStickyPartitioner 와 RoundRobinPartitioner
프로듀서 API 에서는 파티셔너를 지정할 수 있는데, 대표적으로 UniformStickyPartitioner 와 RoundRobinPartitioner 가 있다 
UniformStickyPartitioner 와 RoundRobinPartitioner 는 메시지 키의 해시값과 파티션을 매칭하여 데이터를 전송하는 점은 동일함 
두 파티셔너의 차이점은 **배치를 생성하는 방법**에 차이가 있다 

- RoundRobinPartitioner
RoundRobinPartitioner 는 레코드를 들어오는 대로 파티션을 순회하면서 전송하기 때문에 배치로 묶이는 빈도가 적다 
ㄴ 배치로 처리할 수 있는 레코드의 수가 적은 단점이 있음 
카프카 클라이언트 2.4.0 이전 버전에서 기본 파티셔너 
ㄴ DefaultPartitioner 가 RoundRobinPartitioner 를 사용

- UniformStickyPartitioner
UniformStickyPartitioner 는 RoundRobinPartitioner 의 단점을 개선하기 위해서 등장했다 
UniformStickyPartitioner 는 어큐뮬레이터에서 데이터가 배치로 모두 묶일 떄까지 기다렸다가 배치를 동일한 파티션에 전송하여 향상된 성능을 제공 
카프카 클라이언트 2.4.0 이후 버전에서 기본 파티셔너
ㄴ DefaultPartitioner 가 UniformStickyPartitioner 를 사용

카프카 클라이언트에서는 커스텀 파티셔너 생성을 위한 Partitioner 인터페이스를 제공하며 해당 인터페이스를 implements 하여 구현할 수 있다 

> 추가적으로 프로듀서는 압축 옵션을 통해서 브로커로 전송시 압축 방식을 지정할 수 있다 
> gzip, snappy. lz4, zstd 압축 방식을 지원함
> 압축을 하면 데이터 전송 시 네트워크 처치량의 장점은 있지만, 압축을 하는데에 CPU, 메모리를 사용하기 때문에 압축이 필요한지 고민하여 사용해야 한다 


### 프로듀서 옵션
필수 옵션 
- bootstrap.servers : 카프카 클러스터에 속한 브로커의 호스트:포트 들을 지정 
- key.serializer : 메시지 키 직렬화 클래스
- value.serializer : 메시지 값 직렬화 클래스 
선택 옵션 
- acks : 프로듀서가 전송한 데이터가 브로커에 저장 되었는지 전송 성공여부 확인하는 옵션
    * 1 (default) : 리더 파티션에 데이터가 저장되면 성공으로 판단 
    * 0 : 프로듀서 전송 즉시 성공으로 판단 (실제 저장여부는 검사하지 않음)
    * -1(all) : min.insync.replicas 개수에 해당하는 리더 파티션과 팔로워 파티션에 데이터가 저장되면 성공하는 것으로 판단 
- buffer.memory : 배치로 모으기 위해 설정할 버퍼 메모리 설정 (default: 32MB)
- retries : 프로듀서가 전송 실패시에 재전송 횟수 지정 (default: 2147483647)
- batch.size : 레코드 최대 용량을 지정
    * 너무 작게 설정하면 자주 보내야 하기 때문에 네트워크 부담이 있음 
    * 너무 크게 설정하면 메모리를 더 많이 사용하게 됨 
    * default: 16384
- linger.ms : 배치를 전송하기 전까지 기다리는 최소 시간 (default:0)
- paritioner.class : 파티셔너 클래스를 지정 (default: DefaultPartitioner)
- enable.idempotence : 멱등성 프로듀서로 동작할지 여부를 지정 (default: false)
- transaction.id : 프로듀서가 레코드를 전송할 때 레코드를 트랜잭션 단위로 묶을지 여부를 설정 
    * 프로듀서 고유의 트랜잭션 id 설정 


### 간단한 코틀린 어플리케이션 
```kotlin
private const val BOOTSTRAP_SERVERS = "my-kafka:9092"
const val TOPIC_NAME = "test"

fun createKafkaConfig() : Properties {
    val configs = Properties()
    // 카프카 접속 정보 설정
    configs[ProducerConfig.BOOTSTRAP_SERVERS_CONFIG] = BOOTSTRAP_SERVERS
    // key, value 직렬화 클래스 지정
    configs[ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG] = StringSerializer::class.java
    configs[ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG] = StringSerializer::class.java

    return configs
}

fun produceSimple() {
    val configs = createKafkaConfig()

    // 프로듀서 생성
    val producer = KafkaProducer<String, String>(configs)

    val messageKey = "testKey"
    val messageValue = "testMessage"
    // 전송 단위 record 지정
    val record = ProducerRecord(TOPIC_NAME, messageKey, messageValue)
    // 전송 요청
    producer.send(record)

    // send 는 레코드를 모아서 배치형태로 전송하기 때문에 flush 로 버퍼를 비우도록 .. 전송을 의미함
    producer.flush()
    producer.close()
}
```
- 카프카 클라이언트는 send 메소드를 호출하더라도 바로 레코드를 전송하지 않는다 
- 내부적으로 버퍼가 존재하여 레코드를 모아서 배치형태로 전송한다 > 성능 향상을 위해서 중요함 
- 때문에 위 예제에서 명시적으로 flush 를 호출하여 메시지가 전송되도록 하였다 

### 파티션 번호 지정한 프로듀서 
레코드 전송시 파티션을 지정하고 싶다면 ProducerRecord 생성시에 파티션 번호를 지정하면 된다 
```kotlin
// 파티션 번호 지정 
val partitionNo = 0;
val record = ProducerRecord(TOPIC_NAME, partitionNo, messageKey, messageValue)
```

### 커스텀 파티셔너 구현 
위에서 설명했든 커스텀 파티셔너는 Partitioner 인터페이스를 구현하여 작성할 수 있다 
```kotlin
internal class CustomPartitioner : Partitioner {
    override fun configure(configs: MutableMap<String, *>?) {}

    override fun close() {}

    override fun partition(
        topic: String,
        key: Any?,
        keyBytes: ByteArray?,
        value: Any?,
        valueBytes: ByteArray?,
        cluster: Cluster
    ): Int {
        // 메시지 키가 없는경우 throw
        if (keyBytes == null) {
            throw InvalidRecordException("Need message key")
        }

        // 판교는 1번 파티션으로 하드코딩
        if ("Pangyo".equals(key)) {
            return 0
        }

        // 파티션 크기로 제산법
        val partitions = cluster.partitionsForTopic(topic)
        return Utils.toPositive(Utils.murmur2(keyBytes)) % partitions.size
    }
}

// 커스텀 파티셔너 이용
fun produceWithCustomPartitioner() {
    val configs = createKafkaConfig()
    // 파티셔너 지정
    configs[ProducerConfig.PARTITIONER_CLASS_CONFIG] = CustomPartitioner::class.java

    // 프로듀서 생성
    val producer = KafkaProducer<String, String>(configs)

    val recordPangyo = ProducerRecord(TOPIC_NAME, "Pangyo", "23")
    val recordSeoul = ProducerRecord(TOPIC_NAME, "Seoul", "10")

    producer.send(recordPangyo)
    producer.send(recordSeoul)
    producer.flush()
    producer.close()
}
```

### 브로커에 정상적으로 전송 되었는지 확인하는 프로듀서 
KafkaProducer 의 sned() 메서드는 Future 객체를 반환
이 Future 는 RecordMetadata 의 비동기 결과를 나타내는 것으로 RecordMetadata 에는 브로커에 정상적으로 적재 되었는지에 대한 데이터가 포함됨 
```kotlin
fun produceSyncCheck() {
    val configs = createKafkaConfig()

    // 프로듀서 생성
    val producer = KafkaProducer<String, String>(configs)

    val messageKey = "syncCallbackKey"
    val messageValue = "syncCallbackMsg"
    // 전송 단위 record 지정
    val record = ProducerRecord(TOPIC_NAME, messageKey, messageValue)

    // 전송 요청
    val callbackFuture = producer.send(record)
    val metadata = callbackFuture.get()


    // send 는 레코드를 모아서 배치형태로 전송하기 때문에 flush 로 버퍼를 비우도록 .. 전송을 의미함
    producer.flush()
    producer.close()
}
```

- 레코드가 정상적으로 브로커에 전달되었다면 토픽 이름, 파티션 번호, 오프셋 번호가 출력됨 
- 전송 결과를 동기식으로 확인하는 것은 Future 의 get 은 blocking 을 야기 하기 때문에 부담이 될 수 있다 

카프카는 비동기로 결과 확인을 위해서 callback 인터페이스를 제공한다 
```kotlin
internal class ProducerCallback : Callback {
    private val logger = LoggerFactory.getLogger(this.javaClass)

    override fun onCompletion(metadata: RecordMetadata?, exception: Exception?) {
        if (exception != null) {
            logger.error(exception.message, exception)
        } else {
            logger.info("ProducerCallback metadata: {}", metadata.toString())
        }
    }
}

fun produceAsyncCheck() {
    val configs = createKafkaConfig()

    // 프로듀서 생성
    val producer = KafkaProducer<String, String>(configs)

    val messageKey = "asyncCallbackKey"
    val messageValue = "asyncCallbackMsg"
    // 전송 단위 record 지정
    val record = ProducerRecord(TOPIC_NAME, messageKey, messageValue)

    producer.send(record, ProducerCallback())

    // send 는 레코드를 모아서 배치형태로 전송하기 때문에 flush 로 버퍼를 비우도록 .. 전송을 의미함
    producer.flush()
    producer.close()
}
```










## 컨슈머 API 
컨슈머 애플리케이션은 카프카에 적재된 데이터를 가져와서 필요한 처리를 수행한다 
ex) 문자를 발송하는 컨슈머 어플리케이션 

## 간단한 오토 커밋(autocommit) 컨슈머 어플리케이션 
```kotlin
private const val BOOTSTRAP_SERVERS = "my-kafka:9092"
private const val GROUP_ID = "test-group"
const val TOPIC_NAME = "test"

fun createKafkaConfig() : Properties {
    val configs = Properties()
    // 카프카 접속 정보 설정
    configs[ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG] = BOOTSTRAP_SERVERS
    // 컨슈머 그룹 아이디 지정
    configs[ConsumerConfig.GROUP_ID_CONFIG] = GROUP_ID
    // key, value 역직렬화 클래스 지정
    configs[ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG] = StringSerializer::class.java
    configs[ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG] = StringSerializer::class.java

    return configs
}

class SimpleConsumer
private val logger = LoggerFactory.getLogger(SimpleConsumer::class.java)

// key, value produce 예제
fun consumeSimple() {
    val configs = createKafkaConfig()

    // 컨슈머 생성
    val consumer = KafkaConsumer<String, String>(configs)
    consumer.subscribe(arrayListOf(TOPIC_NAME))

    // 무한 루프를 돌면서 .. 계속 consume
    while (true) {
        // record 가져오기
        val records = consumer.poll(Duration.ofSeconds(1))

        records.forEach {
            logger.info("record : {}", it)
        }

    }
}
```
- 위 예제를 보면 무한 루프를 돌면서 test 토픽을 컨슘한다 
- 이제 프로듀서 콘솔을 이용하여 토픽에 레코드를 전달해본다 
```shell script
$ bin/kafka-console-producer.sh --bootstrap-server my-kafka:9092 \
    --topic test
> testMsg
```
- 로그에 레코드가 출력되는 것을 확인 할 수 있다 
```log
[main] INFO com.example.study.consumer.SimpleConsumer - record : ConsumerRecord(topic = test, partition = 0, leaderEpoch = 0, offset = 4, CreateTime = 1621759508512, serialized key size = -1, serialized value size = 7, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = testMsg)
```

### 컨슈머의 중요 개념 
컨슈머를 운영하는 방법은 크게 2가지 방법이 있음 
- 컨슈머 그룹을 이용하는 방법 
- 토픽의 특정 파티션만 구독하는 컨슈머를 운영 

#### 컨슈머 그룹을 이용하는 방법
컨슈머를 각 컨슈머 그룹으로 부터 격리된 한경에서 안전하게 운영할 수 있도록 하는 카프카의 독특한 방식 
[그림 페이지 98, 99]
- 이전에도 설명했지만 1개의 파티션은 최대 1개의 컨슈머에 할당이 가능
- 1개의 컨슈머는 여러 파티션에 할당 가능 
- 결국 파티션 개수는 컨슈머 개수와 같거나 커야한다 (파티션 수 < 컨슈머 수)
- 컨슈머의 수가 더 많다면 불필요한 스레드로 남게 되기 때문에 이런 상황을 미리 예방해야 함 

그렇다면 카프카에서 컨슈머 그룹을 사용하는 이유는 뭘까 ? 
예를 들어서 데이터를 하둡과 엘라스틱 서치에 함께 저장한다고 가정해보자 

기존의 end-to-end 동기식 방식을 사용한다면 엘라스틱 서치에 저장하는 부분에서 장애가 발생하면 하둡에도 데이터가 저장되지 않을 수 있음 
[그림 페이지 100]

카프카는 이런 문제를 해결하기 위해서 컨슈머 그룹이라는 개념을 도입했다 
[그림 페이지 101]
- 위 그림을 보면 엘라스틱 서치 컨슈머 그룹과 하둡 컨슈머 그룹을 분리해서 데이터를 저장하고 있다 
- 이런 방식을 사용하게 되면 하둡 컨슈머 그룹쪽에 문제가 생기더라도 엘라스틱 서치 컨슈머 그룹은 아무런 영향을 받지 않게 된다 

하둡 컨슈머 그룹의 컨슈머가 장애가 발생하면, 
- 장애가 발생한 컨슈머가 할당되어 있던 파티션은 다른 컨슈머에게 제어권을 넘기게 된다 => 리밸런싱 
- 리밸런싱은 2가지 상황에서 일어나는데, 위에서 설명한 것 처럼 컨슈머가 장애가 발생하거나 (컨슈머가 제거되는 경우) 컨슈머가 추가되는 경우에 발생한다 
- 카프카는 장애 허용 시스템을 위해서 리밸런싱을 도입함 

- 리밸런싱은 컨슈머가 데이터를 처리하는 도중에 언제든지 발생할 수 있기 때문에 개발자는 리밸런싱에 대한 대응 코드를 작성해야함 
- 리밸런싱이 발생하면 컨슈머 그룹의 컨슈머들이 토픽의 데이터를 읽을 수 없음 
- 그룹 조정자는 컨슈머 추가 및 삭제 될 때 리밸런싱을 작동시킨다 


컨슈머는 카프카 브로커에서 어디까지 데이터를 가져갔는지 **커밋**을 통해 기록함 
해당 내용은 _consumer_offsets 토픽에 기록된다 
특정 컨슈머가 장애가 발생하여 커밋에 실패했다면 데이터 처리의 중복이 발생할 수 있음 
그렇기 때문에 개발자는 데이터의 중복 처리 방지를 위해서 오프셋 커밋을 정상적으로 처리했는지 검증해야한다 
[그림 페이지102]

오프셋 커밋은 명시적, 비명시적으로 수행할 수 있는데 기본 값은 비명시적 커밋(autocommit) 이다 
ㄴ poll 메서드가 수행되면 일정 간격마다 오프셋을 커밋함 (enable.auto.commit=true)

비명시적 커밋은 편리하지만, poll 메소드 호출 이후 리밸런싱 or 컨슈머 강제종료 시에 
데이터의 중복처리, 데이터의 유실이 발생할 수 있다 
따라서 데이터의 중복 처리, 유실이 허용되지 않는 시스템에서는 명시적 커밋을 이용해야 한다 

명시적 커밋을 이용하려면 commitSync() 메서드를 호출하여 수행 가능하다 
poll() 메서드를 통해 반환된 레코드의 가장 마지막 오프셋을 기준으로 커밋을 수행함 
commitSync() 메서드는 동기식으로 동작하기 때문에 blocking 이 걸리는데 데이터 처리량에 영향을 준다면 commitAsync() 메서드를 이용할 수 있다 
다만 commitAsync() 는 커밋을 요청하고 이후 데이터를 바로 처리하기 때문에 커밋이 실패할 경우 처리중인 데이터의 순서를 보장하기 어렵고 데이터 중복 문제가 발생할 수 있다 

컨슈머 내부 구조
[그림 페이지 103]
- poll() 메서드를 호출하는 시점에 브로커에서 데이터를 가져오는 것이 아님 
- 컨슈머 어플리케이션을 실행하면 내부에서 Fetcher 인스턴스가 생성되고 
- Fetcher 인스턴스 내부 큐에 미리 레코드들을 받아온다 
- poll() 메서드 호출은 Fetcher 인스턴스의 큐에서 레코드를 가져오는 역할을 수행한다 

### 컨슈머 주요 옵션 
필수 옵션
- bootstrap.servers : 카프카 브로커의 접속정보
- key.deserializer : 메시지 키를 역직렬화 하는 클래스
- value.deserializer : 메시지 값을 역질력화 하는 클래스

선택 옵션
- group.id : 커슈머 그룹의 아이디를 지정. subscrib() 메서드의 필수 값이며 기본 값은 null
- auto.offset.reset: 저장된 컨슈머 오프셋이 없는 경우 어느 오프셋부터 읽을지 선택하는 옵션 
ㄴ latest, earliest, none 에서 지정할 수 있음 
ㄴ latest : 가장 최근에 넣은 오프셋 부터 시작 
ㄴ earliest : 가장 오래된 오프셋 부터 시작 
ㄴ none : 커밋 기록을 찾아서 없으면 에러,  있으면 그때 부터 읽는다 
- enable.auto.commit : 자동 커밋 할지 여부를 설정 
- auto.commit.interval.ms : 자동 커밋 간격 지정 
- max.poll.records : poll() 메서드를 통해 반환되는 레코드의 최대 수 지정 
- session.timeout.ms : 컨슈머와 브로커의 연결이 끊기는 최대 시간 지정 
ㄴ 해당 시간내에 컨슈머가 heartbeat 를 전송하지 않으면 브로커는 컨슈머에 문제가 생겼다고 가정한다 
- heartbeat.interval.ms : 하트 비트를 전송하는 시간 간격 (default: 3000ms)
- max.poll.inverval.ms : poll() 메서드를 호출하는 간격의 최대 시간 
ㄴ 데이터를 처리하는데에 시간이 너무 많이 걸리는 경우 비정상으로 판단해서 리밸런싱 수행 
- isolation.level : 트랜잭션 프로듀서가 트랜잭션 단위로 레코드를 보낼 경우 사용함 
ㄴ read_commited, read_uncommited 가 있으며 
ㄴ read_commited 는 commit 완료된 레코드만 읽는다 
ㄴ read_uncommited 는 커밋 여부에 상관없이 파티션의 처음부터 읽는다 

### 동기 오프셋 커밋 
```kotlin
class OffsetCommitConsumer
private val logger = LoggerFactory.getLogger(OffsetCommitConsumer::class.java)

fun consumeSyncCommit() {
    val configs = createKafkaConfig()
    // auto commit off
    configs[ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG] = false

    val consumer = KafkaConsumer<String, String>(configs)
    consumer.subscribe(arrayListOf(TOPIC_NAME))

    // 무한 루프를 돌면서 .. 계속 consume
    while (true) {
        // record 가져오기
        val records = consumer.poll(Duration.ofSeconds(1))
        
        // key : 파티션 정보 (TopicPartition)
        // value : 오프셋 정보 (OffsetAndMetadata)
        val currentOffset = HashMap<TopicPartition, OffsetAndMetadata>()

        records.forEach {
            logger.info("record : {}", it)
            
            val partition = TopicPartition(it.topic(), it.partition()) 
            currentOffset[partition] = OffsetAndMetadata(it.offset() + 1, null)
            
            // currentOffset 을 전달해야 명시적 commit 이 가능 
            consumer.commitSync(currentOffset)
        }
    }
}
```
- 개별 레코드 단위로 오프셋을 커밋하고 싶다면 commitSync() 호출시 Map<TopicPartition, OffsetAndMetatdata> 파라미터로 전달해야함 
- 위 예제를 보면 각 레코드별로 currentOffset 의 key 인 TopicPartition 을 생성하고 OffsetAndMetatdata 에 offset + 1 을 한뒤 전달하고 있음 
- 컨슈머를 실행시킨 상태에서 test 토픽에 레코드를 produce 하면 레코드가 1개씩 추가되는 것을 알 수 있음 

### 비동기 오프셋 커밋 
동기식 커밋은 커밋 응답을 기다리는 동안 blocking 되기 때문에 데이터 처리가 일시적으로 중단 될 수 있다 
더 많은 데이터를 처리하기 위해서는 비동기 오프셋 커밋을 사용할 수 있다 

동기식 오프셋에서 while 문 내부만 수정 
```kotlin
while (true) {
    // record 가져오기
    val records = consumer.poll(Duration.ofSeconds(1))

    records.forEach {
        logger.info("record : {}", it)

        // 비동기 처리를 위한 callback 작성 
        val callback = object: OffsetCommitCallback {
            override fun onComplete(offsets: MutableMap<TopicPartition, OffsetAndMetadata>, exception: Exception?) {
                if (exception == null) {
                    logger.info("commit success")
                } else {
                    logger.error("commit failed .. offsets: {}", offsets, exception)
                }
            }
        }

        consumer.commitAsync(callback)
    }
}
```
- commitAsync 메소드에 비동기 처리를 위한 OffsetCommitCallback 을 전달하여 commit 비동기로 오프셋을 커밋하고 응답에 대한 처리가 가능 


### 리밸런스 리스너를 가진 컨슈머 
컨슈머 그룹에서 컨슈머 추가, 삭제가 일어나면 파티션을 재할당하는 **리밸런스**가 일어나는데
poll() 메서드를 통해 반환 받은 데이터를 모두 처리하기 전에 리밸런싱이 일어나면 데이터를 중복 처리할 수 있다 
이런 현상은 poll() 로 데이터 일부를 처리했으나 커밋하지 못한 경우 발생한다 

리밸런스 발생 시 데이터를 중복 처리하지 않기 위해서 리밸런스 발생시 처리한 데이터까지는 커밋을 수행해야 함 
이를 위해서 카프카 라이브러리에서는 ConsumerRebalanceListenr 를 지원한다 
```kotlin
private lateinit var consumer : KafkaConsumer<String, String>
private lateinit var currentOffset : HashMap<TopicPartition, OffsetAndMetadata>

fun consumeWithRebalanceListener() {
    val configs = createKafkaConfig()

    // auto commit off
    configs[ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG] = false

    consumer = KafkaConsumer<String, String>(configs)
    consumer.subscribe(arrayListOf(TOPIC_NAME), RebalanceLinstenr())

    // 무한 루프를 돌면서 .. 계속 consume
    while (true) {
        // record 가져오기
        val records = consumer.poll(Duration.ofSeconds(1))

        currentOffset = HashMap<TopicPartition, OffsetAndMetadata>()
        records.forEach {
            logger.info("record : {}", it)

            val partition = TopicPartition(it.topic(), it.partition())
            currentOffset[partition] = OffsetAndMetadata(it.offset() + 1, null)
            consumer.commitSync(currentOffset)
        }
    }
}

internal class RebalanceLinstenr : ConsumerRebalanceListener {
    // 리밸런스가 시작되기 전에 호출됨
    override fun onPartitionsRevoked(partitions: MutableCollection<TopicPartition>?) {
        logger.warn("Partitions are revoked")
        // 리밸런스가 시작되기 전에 저장하고 있던 마지막 정보를 offset 을 커밋
        consumer.commitSync(currentOffset)
    }

    // 리밸런스가 끝난 뒤에 파티션 할당이 완료되면 호출됨
    override fun onPartitionsAssigned(partitions: MutableCollection<TopicPartition>?) {
        logger.warn("Partitions are assigned")
    }
}
```
- RebalanceLinstenr 를 사용하는 예제는 sync 예제와 거의 유사하며 컨슈머의 subscribe 메소드의 2번째 인자로 ConsumerRebalanceListener 구현한 파라미터를 전달하면 된다 
- RebalanceLinstenr 에서는 리밸런스가 일어나기 전에 onPartitionsRevoked() 이 호출되기 때문에 내부에 처리한 레코드의 offset 을 저장하고 있는 currentOffset 을 이용해 커밋 
- 이렇게 하면 데이터 중복 처리를 방지할 수 있다 
> 여기서는 전역 변수 consumer, currentOffset 을 이용했지만, 실제 서비스 개발에서는 다른 방법으로 생성하는 방안을 고려해보아야 할 듯 하다 

### 파티션 할당 컨슈머 
컨슈머 운영시에 subscribe() 대신에 assign() 메소드를 이용해 파티션을 지정하여 컨슈머에 명시적으로 할당할 수 있음 
```kotlin
const val PARTITION_NUMBER = 0
fun consumeWithExactPartition() {
    val configs = createKafkaConfig()

    val consumer = KafkaConsumer<String, String>(configs)
    
    // TopicPartition : 토픽, 파티션 정보 정의 
    val topicPartition = TopicPartition(TOPIC_NAME, PARTITION_NUMBER)
    // 파티션 할당 
    consumer.assign(Collections.singleton(topicPartition))

    while (true) {
        // record 가져오기
        val records = consumer.poll(Duration.ofSeconds(1))

        // key : 파티션 정보 (TopicPartition)
        // value : 오프셋 정보 (OffsetAndMetadata)
        val currentOffset = HashMap<TopicPartition, OffsetAndMetadata>()

        records.forEach {
            logger.info("record : {}", it)
            
        }
    }
}
```
- TopicPartition 을 이용해서 토픽과 파티션 정보를 담은 객체를 생성하고 assign() 에 전달하여 0번 파티션을 할당했다
- 이렇게 assign() 을 통해서 특정 파티션에 할당하면 리밸런싱 과정이 없다 
ㄴ 다만 해당 파티션이 삭제되면 consume 이 불가능하기 때문에 카프카 클러스터 전체를 이렇게 사용하는 것은 어려울 것 같다 (개인의견)

### 컨슈머에 할당된 파티션을 확인하는 방법 
```kotlin
fun checkPartition() {
    val configs = createKafkaConfig()

    val consumer = KafkaConsumer<String, String>(configs)
    consumer.subscribe(arrayListOf(TOPIC_NAME))
    
    val assignedTopicPartition = consumer.assignment()
}
```
- 컨슈머의 assignment() 메소드를 이용해서 할당된 토픽과 파티션 정보를 얻을 수 있다 
- assignment() 는 Set<TopicPartition> 을 반환한다 


### 컨슈머의 안전한 종료 
비정상적으로 종료된 컨슈머는 session timeout 전까지 컨슈머 그룹에 남게되는데, 
종료된 컨슈머는 데이터를 가져오지 못하기 때문에 lag 이 늘어나게 되는 문제가 있다 
이로 인해서 데이터 처리 지연 (실시간이 어렵다는 것을 말하려는 듯..)이 발생한다 
카프카 라이브러리는 컨슈머의 안전한 종료를 지원하기 위해서 wakeup() 메서드를 지원한다 

```kotlin
// 메인 메소드에서 shutdown hook 등록 
fun main() {
    val shutdownThread = Thread {
        logger.info("Shutdown hook")
        consumer.wakeup()
    }
    Runtime.getRuntime().addShutdownHook(shutdownThread)

    consumeWithShutdownHook()
}

// consumeWithShutdownHook 의 일부 구현 
fun consumeWithShutdownHook() {
    // ...
    try {
        while (true) {
            // record 가져오기
            val records = consumer.poll(Duration.ofSeconds(1))

            // key : 파티션 정보 (TopicPartition)
            // value : 오프셋 정보 (OffsetAndMetadata)
            val currentOffset = HashMap<TopicPartition, OffsetAndMetadata>()

            records.forEach {
                logger.info("record : {}", it)

                val partition = TopicPartition(it.topic(), it.partition())
                currentOffset[partition] = OffsetAndMetadata(it.offset() + 1, null)

                // currentOffset 을 전달해야 명시적 commit 이 가능
                consumer.commitSync(currentOffset)
            }
        }
    } catch (e : WakeupException) {
        // poll 요청을 하다가 wakeup() 이 호출되면 WakeupException 발생
        logger.warn("Wakeup consumer")
        // 리소스 종료 처리
    } finally {
        // 카프카 클러스터에 알리기
        consumer.close()
    }
}
```
- wakeup() 으로 컨슈머 인스턴스를 안전하게 종료할 수 있다 
- wakeup() 이후에 poll() 이 호출되면 WakeupException 이 발생함 
- 마지막으로 close() 를 호출해서 카프카 클러스터에 컨슈머가 종료되었음을 알려준다 



## 어드민 API 
